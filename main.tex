% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "guidelines" option to generate the final version.
%\usepackage[guidelines]{nlpreport} % show guidelines
\usepackage[]{nlpreport} % hide guidelines


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs} % for tables






% THE pdfinfo Title AND Author ARE NOT NECESSARY, THEY ARE METADATA FOR THE FINAL PDF FILE
\hypersetup{pdfinfo={
Title={Guidelines and template for NLP coursework report},
Author={Jane Smith \& John Doe}
}}
%\setcounter{secnumdepth}{0}  
 \begin{document}
%
\title{Assignment 2\\
\explanation{\rm Substitute the $\uparrow$ title $\uparrow$ with your project's title, or with Assignment 1 / 2\\ \smallskip}
% subtitle:
\large \explanation{\rm $\downarrow$ Keep only one of the following three  labels  / leave empty for assignments: $\downarrow$\\}
}
\author{Daniele Algeri,
Martina Kenna,
Joana Pimenta
\and
Matilde Simonini\\
Master's Degree in Artificial Intelligence, University of Bologna\\
\{ daniele.algeri, martina.kenna, joana.monteiro, matilde.simonini \}@studio.unibo.it
}
\maketitle


\attention{DO NOT MODIFY THIS TEMPLATE - EXCEPT, OF COURSE FOR TITLE, SUBTITLE AND AUTHORS.\\ IN THE FINAL VERSION, IN THE \LaTeX\ SOURCE REMOVE THE \texttt{guidelines} OPTION FROM  \texttt{$\backslash$usepackage[guidelines]\{nlpreport\}}.
}

\begin{abstract}
%\begin{quote}

\explanation{
The abstract is very brief summary of your report. Try to keep it no longer than 15-20 lines at most. Write your objective, your approach, and your main observations (what are the findings that make this report worthwhile reading?)}


This work compares two open-source LLMs, \href{https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0}{\textit{TinyLlama-1.1B-Chat-v1.0}}  and \href{https://huggingface.co/mistralai/Mistral-7B-v0.3}{\textit{Mistral-7B-Instruct-v0.3}}, on multiclass sexism detection using zero-shot and few-shot prompting. Results highlight a critical dependency on model size: TinyLlama consistently failed to follow instructions in both inference settings and, even after Prompt Tuning, semantic reasoning was still absent, leading to a single-class prediction bias, although formatting errors were corrected. In contrast, Mistral-7B demonstrated effective instruction adherence. Although initially biased towards one class, Mistral's performance improved significantly with few-shot examples, raising the F1-score from 0.35 to 0.46.
%\end{quote}
\end{abstract}

\attention{\textcolor{red}{NOTICE: THIS REPORT'S LENGTH MUST RESPECT THE FOLLOWING PAGE LIMITS: \begin{itemize}
    \item ASSIGNMENT: \textbf{2 PAGES} 
    \item NLP PROJECT OR PROJECT WORK: \textbf{8 PAGES}
    \item COMBINED NLP PROJECT + PW: \textbf{12 PAGES}
\end{itemize}  PLUS LINKS, REFERENCES AND APPENDICES.\\ 
THIS MEANS THAT YOU CANNOT FILL ALL SECTIONS TO MAXIMUM LENGTH. IT ALSO MEANS THAT, QUITE POSSIBLY, YOU WILL HAVE TO LEAVE OUT OF THE REPORT PART OF THE WORK YOU HAVE DONE OR OBSERVATIONS YOU HAVE. THIS IS NORMAL: THE REPORT SHOULD EMPHASIZE WHAT IS MOST SIGNIFICANT, NOTEWORTHY, AND REFER TO THE NOTEBOOK FOR ANYTHING ELSE.\\ 
FOR ANY OTHER ASPECT OF YOUR WORK THAT YOU WOULD LIKE TO EMPHASIZE BUT CANNOT EXPLAIN HERE FOR LACK OF SPACE, FEEL FREE TO ADD COMMENTS IN THE NOTEBOOK.\\ 
INTERESTING TEXT EXAMPLES THAT EXCEED THE MAXIMUM LENGTH OF THE REPORT CAN BE PLACED IN A DEDICATED APPENDIX AFTER THE REFERENCES.}}


\section{Introduction}
\label{sec:introduction}
\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 2 COLUMNS FOR PROJECT OR PW / 3 FOR COMBINED REPORTS.}

\explanation{
The Introduction is an executive summary, which you can think of as an extended abstract.  Start by writing a brief description of the problem you are tackling and why it is important. (Skip it if this is an assignment report).} 

\explanation{Then give a short overview of known/standard/possible approaches to that problems, if any, and what are their advantages/limitations.} 

\explanation{After that, discuss your approach, and motivate why you follow that approach. If you are drawing inspiration from an existing model, study, paper, textbook example, challenge, \dots, be sure to add all the necessary references~\cite{DBLP:journals/corr/abs-2204-02311,DBLP:conf/acl/LorenzoMN22,DBLP:conf/clef/AnticiBIIGR21,DBLP:conf/ijcai/NakovCHAEBPSM21,DBLP:conf/naacl/RottgerVHP22,DBLP:journals/toit/LippiT16}.\footnote{\href{https://en.wikipedia.org/wiki/The_Muppet_Show}{Add only what is relevant.}}}

\explanation{Next, give a brief summary of your experimental setup: how many experiments did you run on which dataset. Last, make a list of the main results or take-home lessons from your work.}

\attention{HERE AND EVERYWHERE ELSE: ALWAYS KEEP IN MIND THAT, CRUCIALLY, WHATEVER TEXT/CODE/FIGURES/IDEAS/... YOU TAKE FROM ELSEWHERE MUST BE CLEARLY IDENTIFIED AND PROPERLY REFERENCED IN THE REPORT.}
%Introduction Body
Traditional approaches for text classification often rely on supervised learning with models like BERT \cite{bertmodel}, which require extensive fine-tuning. Recently, Large Language Models (LLMs) have introduced the paradigm of "prompting," allowing models to perform classification tasks without weight updates, relying instead on in-context learning.
In this work, we investigate the efficacy of LLMs in categorizing a set of texts into five distinct categories: \textit{not-sexist}, or one among four sexist classes, namely \textit{threats}, \textit{derogation}, \textit{animosity}, and \textit{prejudiced discussion}. We compare a lightweight model (TinyLlama-1.1B) against a larger model (Mistral-7B).
Our experimental setup involves classifying a balanced, supervised test set of approximately 300 texts, using two different strategies: \textbf{Zero-Shot}, where the model is given the task description and input text only, and \textbf{Few-Shot}, where the model is provided with the task description and a small set of labeled examples.

In addition, we perform \textbf{Prompt Tuning} to TinyLlama to address its instruction-following limitations.
Our experiment highlights that:

\begin{itemize}
    \item The smaller model (TinyLlama, 1.1B parameters) lacks the capacity to follow the syntactic constraints out-of-the-box, and outputs invalid labels for every instance in the dataset.
    \item Prompt tuning successfully forced such model to output valid labels, but could not induce the reasoning required for correct classification.
    \item Mistral tends to default to the “animosity” class in the zero-shot setting.
\end{itemize}
%end of Introduction Body


%\section{Background}
%\label{sec:background}
\attention{MAX 2 COLUMNS (3 FOR COMBINED REPORTS). DO NOT INCLUDE SECTION IF NO BACKGROUND NECESSARY. OMIT SECTION IN ASSIGNMENT REPORTS.}

\explanation{The Background section is where you briefly provide whatever background information on the domain or challenge you're addressing and/or on the techniques/approaches you're using, that (1) you think is necessary for the reader to understand your work and design choices, and (2) is not something that has been explained to you during the NLP course (to be clear: do NOT repeat explanations of things seen in class, we already know that stuff). If you adapt paragraphs from articles, books, online resources, etc: be sure to clarify which parts are yours and which ones aren't.}

\section{System description}
\label{sec:system}
\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 4 COLUMNS FOR PROJECT OR PW / 6 FOR COMBINED REPORTS.}

\explanation{
Describe the system or systems you have implemented (architectures, pipelines, etc), and used to run your experiments. If you reuse parts of code written by others, be sure to make very clear your original contribution in terms of
\begin{itemize}
    \item architecture: is the architecture your design or did you take it from somewhere else
    \item coding: which parts of code are original or heavily adapted? adapted from existing sources? taken from external sources with minimal adaptations?
\end{itemize}
It is a good idea to add figures to illustrate your pipeline and/or architecture(s)
(see Figure~\ref{fig:architecture})
%
%\begin{figure*}
%    \centering
%    \includegraphics[width=\textwidth]{img/architecture.pdf}
%    \caption{Model architecture}
%    \label{fig:architecture}
%\end{figure*}
}

%System Description Body start

%\subsection{Architecture and Pipeline}
We implemented an inference pipeline based on the Hugging Face \texttt{transformers} library. The pipeline for each example in the dataset consists of the following stages:
\begin{enumerate}
    \item \textbf{Prompt Construction:} We use a strict template defining the role of the model, listing the five valid categories, and instructing the model to output \textit{only} the label.
        \begin{itemize}
            \item \textbf{Zero-Shot:} Instruction + target text.
            \item \textbf{Few-Shot:} Instruction + labeled examples + target text.
        \end{itemize} 
    \item \textbf{Inference:} The prompt is passed to the LLM.
    \item \textbf{Output Parsing:} The raw text response is mapped to numerical IDs (0–4). If the model generates invalid text, the system defaults to \textbf{0} (not-sexist).
\end{enumerate}

%\subsection{Prompt Tuning}
To address TinyLlama limitations, we implemented an additional Prompt Tuning pipeline for such model. This technique involves freezing the pre-trained model backbone and optimizing only a small set of continuous vector embeddings that are prepended to the input (corresponding to approx. 0.006\% of parameters), aiming to align the model with the specific prompt structure.

% System Description Body end

%\section{Data}
%\label{sec:data}
\attention{MAX 2 COLUMNS / 3 FOR COMBINED REPORTS. OMIT SECTION IN ASSIGNMENT REPORTS.}

\explanation{Provide a brief description of your data including some statistics and pointers (references to articles/URLs) to be used to obtain the data. Describe any pre-processing work you did. Links to datasets must be placed later in Section~\ref{sec:links}.}

\section{Experimental setup and results}
\label{sec:results}
\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 3 COLUMNS FOR PROJECT OR PW / 5 FOR COMBINED REPORTS.}

\explanation{
Describe how you set up your experiments: which architectures/configurations you used, which hyper-parameters and what methods used to set them, which optimizers, metrics, etc.
\\
Then, \textbf{use tables} to summarize your your findings (numerical results) in validation and test. If you don't have experience with tables in \LaTeX, you might want to use \href{https://www.tablesgenerator.com/}{\LaTeX table generator} to quickly create a table template.
}
The performance is evaluated on the Macro F1 Score and the Accuracy Score. The following tables summarize the performance metrics obtained with the two models.\\

\begin{table}[h]
\centering
\caption{\textbf{Zero-Shot} Inference Metrics}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Model} & \textbf{F1-Score} & \textbf{Accuracy} \\ \midrule
Mistral        & 0.35              & 0.38              \\
TinyLlama      & 0.07              & 0.20              \\ \bottomrule

\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Few-Shot} Inference Metrics}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Model} & \textbf{F1-Score} & \textbf{Accuracy} \\ \midrule
Mistral        & 0.46              & 0.48              \\
TinyLlama      & 0.07              & 0.20              \\ \bottomrule
\end{tabular}
\end{table}

It is crucial to note that TinyLlama's metrics are artifacts of the pipeline defaulting invalid outputs to class 0. While Prompt Tuning successfully corrected the syntax, it resulted in a semantic collapse where the model explicitly predicted \textit{not-sexist} for nearly all inputs. Consequently, the metrics remained identical, effectively shifting from a fallback-driven default to a learned prediction bias without improving discrimination.


\section{Discussion}
\label{sec:discussion}
\attention{MAX 1.5 COLUMNS FOR ASSIGNMENT REPORTS / 3 COLUMNS FOR PROJECT / 4 FOR COMBINED REPORTS. ADDITIONAL EXAMPLES COULD BE PLACED IN AN APPENDIX AFTER THE REFERENCES IF THEY DO NOT FIT HERE.}


\explanation{
Here you should make your analysis of the results you obtained in your experiments. Your discussion should be structured in two parts: 
\begin{itemize}
    \item discussion of quantitative results (based on the metrics you have identified earlier; compare with baselines);
    \item error analysis: show some examples of odd/wrong/unwanted  outputs; reason about why you are getting those results, elaborate on what could/should be changed in future developments of this work.
\end{itemize}
}

%Discussion Section Body start

Our results show that Mistral achieves a respectable zero-shot performance which is further boosted by few-shot examples (F1 increase from 0.35 to 0.46). TinyLlama, conversely, shows static metrics (Accuracy 0.20, F1 0.07) across all setups, which match the statistical probability of a random classifier for a balanced 5-class dataset.

\subsection{Error Analysis}

The invalid outputs provided by TinyLlama, in both Zero-Shot and Few-Shot Inference, consist in long generative text rather than discrete class labels.

A possible reason for this behavior is that TinyLlama-Chat is trained primarily for conversational settings, %\cite{tinyllamamodelcard}
where verbose and explanatory answers are generally preferred. Even the inclusion of in-context demonstrations (Few-Shot prompting) proved insufficient to enforce syntactic adherence, likely because this technique relies on a model’s ability to perform in-context learning, which is known to scale strongly with model size.

Ultimately, the prompt tuning experiment was successful in making the model learn the syntax, as all 300 examples in the test dataset were classified with a single, valid label; nevertheless, the model still failed to learn the semantics, leading to unsatisfying performance once again. This dual failure—first in formatting, then in discrimination—strongly suggests that the 1.1B parameter size acts as a fundamental bottleneck for the classification performance.

Concerning Mistral, the model presents a strong bias towards the \textit{animosity} class in the Zero-Shot setting, which, however, was significantly mitigated by few-shot prompting, as shown by the two confusion matrices in Figure~\ref{fig:conf_matrix}

\begin{figure}[t] % Forces it to the Top
    \centering
    \begin{lstlisting}[
        basicstyle=\ttfamily\small,
        frame=single,
        escapeinside={(*}{*)}
        % Note: No caption or label here anymore
    ]
(*\textbf{Text:}*) "damn, i bet she likes tall dudes"
(*\textbf{True Label:}*) "not-sexist"
(*\textbf{Predicted:}*) "Sexist Categories Definition:
Threats: The text expresses intent [...]"
    \end{lstlisting}
    
    \caption{Example of invalid TinyLlama output}
    \label{lst:example_output}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.05\linewidth]{conf_matrix.png}
  \caption{Confusion matrices for Minstral in Zero-Shot and Few-Shot Inference.}
  \label{fig:conf_matrix}
\end{figure}


%Discussion Section Body end


\section{Conclusion}
\label{sec:conclusion}
\attention{MAX 1 COLUMN.}

\explanation{
In one or two paragraphs, recap your work and main results.
What did you observe? 
Did all go according to expectations? 
Was there anything surprising or worthwhile mentioning?
After that, discuss the main limitations of the solution you have implemented, and indicate promising directions for future improvement.
}

%Conlcusion Section Body start

In this experiment, we evaluated the capabilities of LLMs for multiclass sexism detection. We observed that model size is a decisive factor not only for rigid instruction following (syntax) but also for the semantic understanding required to distinguish between complex classes. TinyLlama-1.1B proved unable to adhere to the output format in standard prompting; furthermore, even when prompt tuning successfully enforced syntactic compliance, the model lacked the representational depth to discriminate between categories, collapsing into a single-class prediction.

In contrast, Mistral-7B demonstrated that a sufficiently large model can effectively interpret constraints and leverage context. The transition from zero-shot to few-shot proved highly effective for Mistral, rectifying a strong bias toward generic "animosity" and allowing for better identification of specific categories like "threats." This confirms that larger models possess the necessary capacity to refine decision boundaries via in-context learning—a capability that appears absent in the smaller 1.1B architecture.

%Conclusion Section Body end

%\section{Links to external resources}
\label{sec:links}
\attention{THIS SECTION IS OPTIONAL}
\explanation{
Insert here:
\begin{itemize}
    \item a link to your GitHub or any other public repo where one can find your code (only if you did not submit your code on Virtuale); 
    \item a link to your dataset (only for non-standard projects or project works).
\end{itemize}
}

\attention{DO NOT INSERT CODE IN THIS REPORT}




\bibliography{nlpreport.bib}
\end{document}